{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naman500/RL-21csu438/blob/main/Exp_7_PolicyIteration_(RL).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the MDP (Transition probabilities, rewards, discount factor)\n",
        "num_states = 11\n",
        "num_actions = 4\n",
        "\n",
        "# Define the state space\n",
        "states = [(i, j) for i in range(3) for j in range(4) if not (i == 1 and j == 1)]\n",
        "\n",
        "# Define the policy\n",
        "policy = {\n",
        "    (0, 0): 'Right',\n",
        "    (0, 1): 'Right',\n",
        "    (0, 2): 'Right',\n",
        "    (1, 0): 'Up',\n",
        "    (1, 2): 'Up',\n",
        "    (2, 0): 'Up',\n",
        "    (2, 1): 'Right',\n",
        "    (2, 2): 'Up',\n",
        "    (2, 3): 'Left',\n",
        "}\n",
        "\n",
        "# Define the rewards\n",
        "rewards = {\n",
        "    (0, 3): 1,\n",
        "    (1, 3): -1,\n",
        "}\n",
        "\n",
        "# Define the transition probabilities\n",
        "def transition_probabilities(state, action):\n",
        "    i, j = state\n",
        "    if state in rewards:\n",
        "        return [(state, 1.0)]  # Terminal state\n",
        "    if action == 'Up':\n",
        "        next_state = (max(i - 1, 0), j)\n",
        "    elif action == 'Down':\n",
        "        next_state = (min(i + 1, 2), j)\n",
        "    elif action == 'Left':\n",
        "        next_state = (i, max(j - 1, 0))\n",
        "    elif action == 'Right':\n",
        "        next_state = (i, min(j + 1, 3))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid action\")\n",
        "    if next_state not in states:\n",
        "        return [(state, 1.0)]  # Stay in the current state if the action leads to an invalid state\n",
        "    return [(next_state, 1.0)]\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "def policy_evaluation(policy, states, rewards, gamma, theta):\n",
        "    V = {state: 0 for state in states}\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in states:\n",
        "            v = V[state]\n",
        "            action = policy.get(state)\n",
        "            if action:\n",
        "                transitions = transition_probabilities(state, action)\n",
        "                V[state] = sum(prob * (rewards.get(next_state, 0) + gamma * V.get(next_state, 0)) for next_state, prob in transitions)\n",
        "            delta = max(delta, abs(v - V[state]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "        # Print the current value matrix\n",
        "        print(f\"Iteration {iteration} - Value Matrix:\")\n",
        "        value_matrix = np.zeros((3, 4))\n",
        "        for state, value in V.items():\n",
        "            i, j = state\n",
        "            value_matrix[i][j] = value\n",
        "        print(value_matrix)\n",
        "        iteration += 1\n",
        "    return V\n",
        "\n",
        "def policy_improvement(policy, V, states, rewards, gamma):\n",
        "    policy_stable = True\n",
        "    for state in states:\n",
        "        old_action = policy.get(state)\n",
        "        if old_action:\n",
        "            action_values = {}\n",
        "            for action in ['Up', 'Down', 'Left', 'Right']:\n",
        "                transitions = transition_probabilities(state, action)\n",
        "                action_values[action] = sum(prob * (rewards.get(next_state, 0) + gamma * V.get(next_state, 0)) for next_state, prob in transitions)\n",
        "            best_action = max(action_values, key=action_values.get)\n",
        "            policy[state] = best_action\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "    return policy_stable\n",
        "\n",
        "theta = 0.0001\n",
        "\n",
        "while True:\n",
        "    V = policy_evaluation(policy, states, rewards, gamma, theta)\n",
        "    if policy_improvement(policy, V, states, rewards, gamma):\n",
        "        break\n",
        "\n",
        "# Print the final policy and value function\n",
        "print(\"\\nOptimal Policy:\")\n",
        "for state, action in policy.items():\n",
        "    print(f\"State {state}: {action}\")\n",
        "print(\"\\nOptimal Value Function:\")\n",
        "for state, value in V.items():\n",
        "    print(f\"State {state}: {value:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKj3xCxqvM3v",
        "outputId": "15dbf83e-4724-4b8e-d88a-1680f525363d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 - Value Matrix:\n",
            "[[0.    0.    1.    0.   ]\n",
            " [0.    0.    0.9   0.   ]\n",
            " [0.    0.    0.81  0.729]]\n",
            "Iteration 1 - Value Matrix:\n",
            "[[0.    0.9   1.    0.   ]\n",
            " [0.    0.    0.9   0.   ]\n",
            " [0.    0.729 0.81  0.729]]\n",
            "Iteration 2 - Value Matrix:\n",
            "[[0.81   0.9    1.     0.    ]\n",
            " [0.729  0.     0.9    0.    ]\n",
            " [0.6561 0.729  0.81   0.729 ]]\n",
            "\n",
            "Optimal Policy:\n",
            "State (0, 0): Right\n",
            "State (0, 1): Right\n",
            "State (0, 2): Right\n",
            "State (1, 0): Up\n",
            "State (1, 2): Up\n",
            "State (2, 0): Up\n",
            "State (2, 1): Right\n",
            "State (2, 2): Up\n",
            "State (2, 3): Left\n",
            "\n",
            "Optimal Value Function:\n",
            "State (0, 0): 0.81\n",
            "State (0, 1): 0.90\n",
            "State (0, 2): 1.00\n",
            "State (0, 3): 0.00\n",
            "State (1, 0): 0.73\n",
            "State (1, 2): 0.90\n",
            "State (1, 3): 0.00\n",
            "State (2, 0): 0.66\n",
            "State (2, 1): 0.73\n",
            "State (2, 2): 0.81\n",
            "State (2, 3): 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the MDP (Transition probabilities, rewards, discount factor)\n",
        "num_states = 11\n",
        "num_actions = 4\n",
        "\n",
        "# Define the state space\n",
        "states = [(i, j) for i in range(3) for j in range(4) if not (i == 1 and j == 1)]\n",
        "\n",
        "# Define the policy\n",
        "policy = {\n",
        "    (0, 0): 'Down',\n",
        "    (0, 1): 'Left',\n",
        "    (0, 2): 'Right',\n",
        "    (1, 0): 'Up',\n",
        "    (1, 2): 'Down',\n",
        "    (2, 0): 'Right',\n",
        "    (2, 1): 'Left',\n",
        "    (2, 2): 'Right',\n",
        "    (2, 3): 'Left',\n",
        "}\n",
        "\n",
        "# Define the rewards\n",
        "rewards = {\n",
        "    (0, 3): 1,\n",
        "    (1, 3): -1,\n",
        "}\n",
        "\n",
        "# Define the transition probabilities\n",
        "def transition_probabilities(state, action):\n",
        "    i, j = state\n",
        "    if state in rewards:\n",
        "        return [(state, 1.0)]  # Terminal state\n",
        "    if action == 'Up':\n",
        "        next_state = (max(i - 1, 0), j)\n",
        "    elif action == 'Down':\n",
        "        next_state = (min(i + 1, 2), j)\n",
        "    elif action == 'Left':\n",
        "        next_state = (i, max(j - 1, 0))\n",
        "    elif action == 'Right':\n",
        "        next_state = (i, min(j + 1, 3))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid action\")\n",
        "    if next_state not in states:\n",
        "        return [(state, 1.0)]  # Stay in the current state if the action leads to an invalid state\n",
        "    return [(next_state, 1.0)]\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "def policy_evaluation(policy, states, rewards, gamma, theta):\n",
        "    V = {state: 0 for state in states}\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in states:\n",
        "            v = V[state]\n",
        "            action = policy.get(state)\n",
        "            if action:\n",
        "                transitions = transition_probabilities(state, action)\n",
        "                V[state] = sum(prob * (rewards.get(next_state, 0) + gamma * V.get(next_state, 0)) for next_state, prob in transitions)\n",
        "            delta = max(delta, abs(v - V[state]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "        # Print the current value matrix\n",
        "        print(f\"Iteration {iteration} - Value Matrix:\")\n",
        "        value_matrix = np.zeros((3, 4))\n",
        "        for state, value in V.items():\n",
        "            i, j = state\n",
        "            value_matrix[i][j] = value\n",
        "        print(value_matrix)\n",
        "        iteration += 1\n",
        "    return V\n",
        "\n",
        "def policy_improvement(policy, V, states, rewards, gamma):\n",
        "    policy_stable = True\n",
        "    for state in states:\n",
        "        old_action = policy.get(state)\n",
        "        if old_action:\n",
        "            action_values = {}\n",
        "            for action in ['Up', 'Down', 'Left', 'Right']:\n",
        "                transitions = transition_probabilities(state, action)\n",
        "                action_values[action] = sum(prob * (rewards.get(next_state, 0) + gamma * V.get(next_state, 0)) for next_state, prob in transitions)\n",
        "            best_action = max(action_values, key=action_values.get)\n",
        "            policy[state] = best_action\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "    return policy_stable\n",
        "\n",
        "theta = 0.0001\n",
        "\n",
        "while True:\n",
        "    V = policy_evaluation(policy, states, rewards, gamma, theta)\n",
        "    if policy_improvement(policy, V, states, rewards, gamma):\n",
        "        break\n",
        "\n",
        "# Print the final policy and value function\n",
        "print(\"\\nOptimal Policy:\")\n",
        "for state, action in policy.items():\n",
        "    print(f\"State {state}: {action}\")\n",
        "print(\"\\nOptimal Value Function:\")\n",
        "for state, value in V.items():\n",
        "    print(f\"State {state}: {value:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHRMX6HJzkD5",
        "outputId": "237ad35c-1487-44f0-c87f-c07cc60bc4e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 - Value Matrix:\n",
            "[[0. 0. 1. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "Iteration 0 - Value Matrix:\n",
            "[[0.   0.   1.   0.  ]\n",
            " [0.   0.   0.9  0.  ]\n",
            " [0.   0.   0.81 0.  ]]\n",
            "Iteration 1 - Value Matrix:\n",
            "[[0.   0.9  1.   0.  ]\n",
            " [0.   0.   0.9  0.  ]\n",
            " [0.   0.   0.81 0.  ]]\n",
            "Iteration 0 - Value Matrix:\n",
            "[[0.    0.    1.    0.   ]\n",
            " [0.    0.    0.9   0.   ]\n",
            " [0.    0.    0.81  0.729]]\n",
            "Iteration 1 - Value Matrix:\n",
            "[[0.    0.9   1.    0.   ]\n",
            " [0.    0.    0.9   0.   ]\n",
            " [0.    0.729 0.81  0.729]]\n",
            "Iteration 2 - Value Matrix:\n",
            "[[0.81   0.9    1.     0.    ]\n",
            " [0.729  0.     0.9    0.    ]\n",
            " [0.6561 0.729  0.81   0.729 ]]\n",
            "\n",
            "Optimal Policy:\n",
            "State (0, 0): Right\n",
            "State (0, 1): Right\n",
            "State (0, 2): Right\n",
            "State (1, 0): Up\n",
            "State (1, 2): Up\n",
            "State (2, 0): Up\n",
            "State (2, 1): Right\n",
            "State (2, 2): Up\n",
            "State (2, 3): Left\n",
            "\n",
            "Optimal Value Function:\n",
            "State (0, 0): 0.81\n",
            "State (0, 1): 0.90\n",
            "State (0, 2): 1.00\n",
            "State (0, 3): 0.00\n",
            "State (1, 0): 0.73\n",
            "State (1, 2): 0.90\n",
            "State (1, 3): 0.00\n",
            "State (2, 0): 0.66\n",
            "State (2, 1): 0.73\n",
            "State (2, 2): 0.81\n",
            "State (2, 3): 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the MDP (Transition probabilities, rewards, discount factor)\n",
        "num_states = 11\n",
        "num_actions = 4\n",
        "\n",
        "# Define the state space\n",
        "states = [(i, j) for i in range(3) for j in range(4) if not (i == 1 and j == 1)]\n",
        "\n",
        "# Define the policy\n",
        "policy = {\n",
        "    (0, 0): ['Right', 'Down'],\n",
        "    (0, 1): ['Right', 'Left'],\n",
        "    (0, 2): ['Right', 'Left', 'Down'],\n",
        "    (1, 0): ['Up', 'Down'],\n",
        "    (1, 2): ['Up', 'Right', 'Down'],\n",
        "    (2, 0): ['Up', 'Right'],\n",
        "    (2, 1): ['Right', 'Left'],\n",
        "    (2, 2): ['Up', 'Down', 'Left', 'Right'],\n",
        "    (2, 3): ['Left', 'Up'],\n",
        "}\n",
        "\n",
        "# Define the rewards\n",
        "rewards = {\n",
        "    (0, 3): 1,\n",
        "    (1, 3): -1,\n",
        "}\n",
        "\n",
        "# Define the transition probabilities\n",
        "def transition_probabilities(state, action):\n",
        "    i, j = state\n",
        "    if state in rewards:\n",
        "        return [(state, 1.0)]  # Terminal state\n",
        "    if action == 'Up':\n",
        "        next_state = (max(i - 1, 0), j)\n",
        "    elif action == 'Down':\n",
        "        next_state = (min(i + 1, 2), j)\n",
        "    elif action == 'Left':\n",
        "        next_state = (i, max(j - 1, 0))\n",
        "    elif action == 'Right':\n",
        "        next_state = (i, min(j + 1, 3))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid action\")\n",
        "    if next_state not in states:\n",
        "        return [(state, 1.0)]  # Stay in the current state if the action leads to an invalid state\n",
        "    return [(next_state, 1.0)]\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "def policy_evaluation(policy, states, rewards, gamma, theta):\n",
        "    V = {state: 0 for state in states}\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in states:\n",
        "            v = V[state]\n",
        "            actions = policy.get(state, [])\n",
        "            if actions:\n",
        "                expected_values = []\n",
        "                for action in actions:\n",
        "                    transitions = transition_probabilities(state, action)\n",
        "                    expected_value = sum(prob * (rewards.get(next_state, 0) + gamma * V.get(next_state, 0)) for next_state, prob in transitions)\n",
        "                    expected_values.append(expected_value)\n",
        "                V[state] = max(expected_values)\n",
        "            delta = max(delta, abs(v - V[state]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "        # Print the current value matrix\n",
        "        print(f\"\\nIteration {iteration} - Value Matrix:\")\n",
        "        value_matrix = np.zeros((3, 4))\n",
        "        for state, value in V.items():\n",
        "            i, j = state\n",
        "            value_matrix[i][j] = value\n",
        "        print(value_matrix)\n",
        "        iteration += 1\n",
        "    return V\n",
        "\n",
        "def policy_improvement(policy, V, states, rewards, gamma):\n",
        "    policy_stable = True\n",
        "    for state in states:\n",
        "        old_actions = policy.get(state, [])\n",
        "        if old_actions:\n",
        "            action_values = {}\n",
        "            for action in ['Up', 'Down', 'Left', 'Right']:\n",
        "                transitions = transition_probabilities(state, action)\n",
        "                expected_value = sum(prob * (rewards.get(next_state, 0) + gamma * V.get(next_state, 0)) for next_state, prob in transitions)\n",
        "                action_values[action] = expected_value\n",
        "            best_actions = [action for action in ['Up', 'Down', 'Left', 'Right'] if action_values[action] == max(action_values.values())]\n",
        "            policy[state] = best_actions\n",
        "            if set(old_actions) != set(best_actions):\n",
        "                policy_stable = False\n",
        "    return policy_stable\n",
        "\n",
        "theta = 0.0001\n",
        "\n",
        "while True:\n",
        "    V = policy_evaluation(policy, states, rewards, gamma, theta)\n",
        "    if policy_improvement(policy, V, states, rewards, gamma):\n",
        "        break\n",
        "\n",
        "# Print the final policy and optimal value function\n",
        "print(\"\\nOptimal Policy:\")\n",
        "for state, actions in policy.items():\n",
        "    print(f\"State {state}: {actions}\")\n",
        "print(\"\\nOptimal Value Function:\")\n",
        "for state, value in V.items():\n",
        "    print(f\"State {state}: {value:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpXGfTjAvjqD",
        "outputId": "d6f7bcd7-413c-452b-f6bd-bfbac4753260"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 0 - Value Matrix:\n",
            "[[0.    0.    1.    0.   ]\n",
            " [0.    0.    0.9   0.   ]\n",
            " [0.    0.    0.81  0.729]]\n",
            "\n",
            "Iteration 1 - Value Matrix:\n",
            "[[0.    0.9   1.    0.   ]\n",
            " [0.    0.    0.9   0.   ]\n",
            " [0.    0.729 0.81  0.729]]\n",
            "\n",
            "Iteration 2 - Value Matrix:\n",
            "[[0.81   0.9    1.     0.    ]\n",
            " [0.729  0.     0.9    0.    ]\n",
            " [0.6561 0.729  0.81   0.729 ]]\n",
            "\n",
            "Iteration 0 - Value Matrix:\n",
            "[[0.    0.    1.    0.   ]\n",
            " [0.    0.    0.9   0.   ]\n",
            " [0.    0.    0.81  0.729]]\n",
            "\n",
            "Iteration 1 - Value Matrix:\n",
            "[[0.    0.9   1.    0.   ]\n",
            " [0.    0.    0.9   0.   ]\n",
            " [0.    0.729 0.81  0.729]]\n",
            "\n",
            "Iteration 2 - Value Matrix:\n",
            "[[0.81   0.9    1.     0.    ]\n",
            " [0.729  0.     0.9    0.    ]\n",
            " [0.6561 0.729  0.81   0.729 ]]\n",
            "\n",
            "Optimal Policy:\n",
            "State (0, 0): ['Right']\n",
            "State (0, 1): ['Right']\n",
            "State (0, 2): ['Right']\n",
            "State (1, 0): ['Up']\n",
            "State (1, 2): ['Up']\n",
            "State (2, 0): ['Up', 'Right']\n",
            "State (2, 1): ['Right']\n",
            "State (2, 2): ['Up']\n",
            "State (2, 3): ['Left']\n",
            "\n",
            "Optimal Value Function:\n",
            "State (0, 0): 0.81\n",
            "State (0, 1): 0.90\n",
            "State (0, 2): 1.00\n",
            "State (0, 3): 0.00\n",
            "State (1, 0): 0.73\n",
            "State (1, 2): 0.90\n",
            "State (1, 3): 0.00\n",
            "State (2, 0): 0.66\n",
            "State (2, 1): 0.73\n",
            "State (2, 2): 0.81\n",
            "State (2, 3): 0.73\n"
          ]
        }
      ]
    }
  ]
}